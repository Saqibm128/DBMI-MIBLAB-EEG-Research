{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train, train_labels = pkl.load(open(\"/n/scratch2/ms994/encoded_train_cnn_lstm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_valid, valid_labels = pkl.load(open(\"/n/scratch2/ms994/encoded_valid_cnn_lstm.pkl\", \"rb\"))\n",
    "encoded_test, test_labels = pkl.load(open(\"/n/scratch2/ms994/encoded_test_cnn_lstm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seizure_detect = test_labels[0]\n",
    "#bug from earlier step, forgot to iterate on length of valid or test, used train instead, with repeat on\n",
    "valid_seizure_detect =  valid_labels[0][:len(encoded_valid)]\n",
    "test_seizure_detect = test_labels[0][:len(encoded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203776, 125, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a quick network here\n",
    "Train on the encoded feature vector for predicting seizure and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'leaky_re_lu_3/LeakyRelu:0' shape=(?, 32) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keras.layers.Input((125*32,))\n",
    "y = keras.layers.Reshape((125,32))(x)\n",
    "for i in range(2):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.CuDNNLSTM(32, return_sequences=True)(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "y = keras.layers.Flatten()(y)\n",
    "for i in range(2):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.Dense(512)(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "y = keras.layers.BatchNormalization()(y)\n",
    "y = keras.layers.Dense(2, activation=\"softmax\")(y)\n",
    "model = keras.models.Model(inputs=x, outputs=y)\n",
    "model.compile(keras.optimizers.Adam(lr=0.00005), loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.layers.core.SpatialDropout1D"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, 125, 32)           8448      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, 125, 32)           8448      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 4000)              16000     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               2048512   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 2,349,442\n",
      "Trainable params: 2,339,266\n",
      "Non-trainable params: 10,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "gen = imblearn.keras.BalancedBatchGenerator(encoded_train.reshape((-1,125*32)), keras.utils.to_categorical(train_labels[0], 2), )\n",
    "val_gen = imblearn.keras.BalancedBatchGenerator(encoded_valid.reshape((-1,32*125)), keras.utils.to_categorical(valid_labels[0][:len(encoded_valid)], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "962/962 [==============================] - 26s 27ms/step - loss: 0.9556 - val_loss: 0.7187\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71866, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 2/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.8742 - val_loss: 0.7176\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71866 to 0.71756, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 3/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.8337 - val_loss: 0.7064\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71756 to 0.70640, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 4/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.8076 - val_loss: 0.7045\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.70640 to 0.70450, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 5/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.7760 - val_loss: 0.7031\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.70450 to 0.70312, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 6/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.7525 - val_loss: 0.7017\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.70312 to 0.70169, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 7/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.7394 - val_loss: 0.7030\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.70169\n",
      "Epoch 8/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.7257 - val_loss: 0.7006\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70169 to 0.70056, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\n",
      "Epoch 9/1000\n",
      "962/962 [==============================] - 21s 22ms/step - loss: 0.7099 - val_loss: 0.7036\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.70056\n",
      "Epoch 10/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.7023 - val_loss: 0.7030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.70056\n",
      "Epoch 11/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6933 - val_loss: 0.7028\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70056\n",
      "Epoch 12/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6868 - val_loss: 0.7054\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70056\n",
      "Epoch 13/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.6792 - val_loss: 0.7079\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.70056\n",
      "Epoch 14/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6763 - val_loss: 0.7082\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.70056\n",
      "Epoch 15/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6665 - val_loss: 0.7109\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.70056\n",
      "Epoch 16/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6623 - val_loss: 0.7175\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.70056\n",
      "Epoch 17/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6574 - val_loss: 0.7209\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.70056\n",
      "Epoch 18/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6516 - val_loss: 0.7221\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.70056\n",
      "Epoch 19/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6436 - val_loss: 0.7290\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.70056\n",
      "Epoch 20/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6375 - val_loss: 0.7345\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.70056\n",
      "Epoch 21/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6316 - val_loss: 0.7379\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.70056\n",
      "Epoch 22/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6256 - val_loss: 0.7446\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.70056\n",
      "Epoch 23/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6168 - val_loss: 0.7486\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.70056\n",
      "Epoch 24/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.6082 - val_loss: 0.7611\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.70056\n",
      "Epoch 25/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5987 - val_loss: 0.7635\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.70056\n",
      "Epoch 26/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5938 - val_loss: 0.7736\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.70056\n",
      "Epoch 27/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5884 - val_loss: 0.7783\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.70056\n",
      "Epoch 28/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5771 - val_loss: 0.7913\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.70056\n",
      "Epoch 29/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5694 - val_loss: 0.8020\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.70056\n",
      "Epoch 30/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5603 - val_loss: 0.8019\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.70056\n",
      "Epoch 31/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5515 - val_loss: 0.8067\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.70056\n",
      "Epoch 32/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5456 - val_loss: 0.8244\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.70056\n",
      "Epoch 33/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5365 - val_loss: 0.8361\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.70056\n",
      "Epoch 34/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5306 - val_loss: 0.8323\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.70056\n",
      "Epoch 35/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5208 - val_loss: 0.8437\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.70056\n",
      "Epoch 36/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5148 - val_loss: 0.8545\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.70056\n",
      "Epoch 37/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.5044 - val_loss: 0.8635\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.70056\n",
      "Epoch 38/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4982 - val_loss: 0.8650\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.70056\n",
      "Epoch 39/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4849 - val_loss: 0.8811\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.70056\n",
      "Epoch 40/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4774 - val_loss: 0.8936\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.70056\n",
      "Epoch 41/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4709 - val_loss: 0.8932\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.70056\n",
      "Epoch 42/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4692 - val_loss: 0.8994\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.70056\n",
      "Epoch 43/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4554 - val_loss: 0.9229\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.70056\n",
      "Epoch 44/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4551 - val_loss: 0.9229\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.70056\n",
      "Epoch 45/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4435 - val_loss: 0.9338\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.70056\n",
      "Epoch 46/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4365 - val_loss: 0.9439\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.70056\n",
      "Epoch 47/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4281 - val_loss: 0.9575\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.70056\n",
      "Epoch 48/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4276 - val_loss: 0.9647\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.70056\n",
      "Epoch 49/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4230 - val_loss: 0.9664\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.70056\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4133 - val_loss: 0.9750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.70056\n",
      "Epoch 51/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.4021 - val_loss: 0.9966\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.70056\n",
      "Epoch 52/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3959 - val_loss: 1.0044\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.70056\n",
      "Epoch 53/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3980 - val_loss: 1.0075\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.70056\n",
      "Epoch 54/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3909 - val_loss: 1.0184\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.70056\n",
      "Epoch 55/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.3839 - val_loss: 1.0398\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.70056\n",
      "Epoch 56/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3796 - val_loss: 1.0448\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.70056\n",
      "Epoch 57/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3756 - val_loss: 1.0298\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.70056\n",
      "Epoch 58/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3729 - val_loss: 1.0384\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.70056\n",
      "Epoch 59/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3644 - val_loss: 1.0646\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.70056\n",
      "Epoch 60/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3632 - val_loss: 1.0763\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.70056\n",
      "Epoch 61/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3517 - val_loss: 1.0787\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.70056\n",
      "Epoch 62/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3465 - val_loss: 1.0956\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.70056\n",
      "Epoch 63/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3454 - val_loss: 1.1028\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.70056\n",
      "Epoch 64/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3400 - val_loss: 1.1121\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.70056\n",
      "Epoch 65/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3364 - val_loss: 1.1147\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.70056\n",
      "Epoch 66/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3307 - val_loss: 1.1342\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.70056\n",
      "Epoch 67/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3285 - val_loss: 1.1306\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.70056\n",
      "Epoch 68/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3241 - val_loss: 1.1411\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.70056\n",
      "Epoch 69/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3171 - val_loss: 1.1419\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.70056\n",
      "Epoch 70/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3122 - val_loss: 1.1528\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.70056\n",
      "Epoch 71/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3112 - val_loss: 1.1574\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.70056\n",
      "Epoch 72/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3053 - val_loss: 1.1707\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.70056\n",
      "Epoch 73/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.3047 - val_loss: 1.1786\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.70056\n",
      "Epoch 74/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2944 - val_loss: 1.1915\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.70056\n",
      "Epoch 75/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2944 - val_loss: 1.2015\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.70056\n",
      "Epoch 76/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2902 - val_loss: 1.2218\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.70056\n",
      "Epoch 77/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2892 - val_loss: 1.2254\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.70056\n",
      "Epoch 78/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2853 - val_loss: 1.2214\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.70056\n",
      "Epoch 79/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2825 - val_loss: 1.2332\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.70056\n",
      "Epoch 80/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2782 - val_loss: 1.2231\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.70056\n",
      "Epoch 81/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2779 - val_loss: 1.2485\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.70056\n",
      "Epoch 82/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2730 - val_loss: 1.2590\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.70056\n",
      "Epoch 83/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2685 - val_loss: 1.2687\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.70056\n",
      "Epoch 84/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2726 - val_loss: 1.2580\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.70056\n",
      "Epoch 85/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2653 - val_loss: 1.2839\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.70056\n",
      "Epoch 86/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2608 - val_loss: 1.2793\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.70056\n",
      "Epoch 87/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2599 - val_loss: 1.3004\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.70056\n",
      "Epoch 88/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2563 - val_loss: 1.2933\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.70056\n",
      "Epoch 89/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2520 - val_loss: 1.3108\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.70056\n",
      "Epoch 90/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2453 - val_loss: 1.3436\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.70056\n",
      "Epoch 91/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2515 - val_loss: 1.3522\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.70056\n",
      "Epoch 92/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2446 - val_loss: 1.3360\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.70056\n",
      "Epoch 93/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2455 - val_loss: 1.3403\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.70056\n",
      "Epoch 94/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2391 - val_loss: 1.3450\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.70056\n",
      "Epoch 95/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2398 - val_loss: 1.3667\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.70056\n",
      "Epoch 96/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2364 - val_loss: 1.3320\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.70056\n",
      "Epoch 97/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2314 - val_loss: 1.3709\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.70056\n",
      "Epoch 98/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2298 - val_loss: 1.3687\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.70056\n",
      "Epoch 99/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2267 - val_loss: 1.3898\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.70056\n",
      "Epoch 100/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2323 - val_loss: 1.3787\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.70056\n",
      "Epoch 101/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2232 - val_loss: 1.3839\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.70056\n",
      "Epoch 102/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2226 - val_loss: 1.4095\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.70056\n",
      "Epoch 103/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2200 - val_loss: 1.4151\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.70056\n",
      "Epoch 104/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2169 - val_loss: 1.4433\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.70056\n",
      "Epoch 105/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2176 - val_loss: 1.4382\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.70056\n",
      "Epoch 106/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2196 - val_loss: 1.4281\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.70056\n",
      "Epoch 107/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2097 - val_loss: 1.4732\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.70056\n",
      "Epoch 108/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2082 - val_loss: 1.4612\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.70056\n",
      "Epoch 109/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2068 - val_loss: 1.4549\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.70056\n",
      "Epoch 110/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2050 - val_loss: 1.4819\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.70056\n",
      "Epoch 111/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1966 - val_loss: 1.4824\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.70056\n",
      "Epoch 112/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2007 - val_loss: 1.5032\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.70056\n",
      "Epoch 113/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.2011 - val_loss: 1.5038\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.70056\n",
      "Epoch 114/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1963 - val_loss: 1.4894\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.70056\n",
      "Epoch 115/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1950 - val_loss: 1.5213\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.70056\n",
      "Epoch 116/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1977 - val_loss: 1.4941\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.70056\n",
      "Epoch 117/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1918 - val_loss: 1.5147\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.70056\n",
      "Epoch 118/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1875 - val_loss: 1.5457\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.70056\n",
      "Epoch 119/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1841 - val_loss: 1.5446\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.70056\n",
      "Epoch 120/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1910 - val_loss: 1.5919\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.70056\n",
      "Epoch 121/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1842 - val_loss: 1.5781\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.70056\n",
      "Epoch 122/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1870 - val_loss: 1.5487\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.70056\n",
      "Epoch 123/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1812 - val_loss: 1.5630\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.70056\n",
      "Epoch 124/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1880 - val_loss: 1.5606\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.70056\n",
      "Epoch 125/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1805 - val_loss: 1.5539\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.70056\n",
      "Epoch 126/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1766 - val_loss: 1.6081\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.70056\n",
      "Epoch 127/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1786 - val_loss: 1.6039\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.70056\n",
      "Epoch 128/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1706 - val_loss: 1.6244\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.70056\n",
      "Epoch 129/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1734 - val_loss: 1.6334\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.70056\n",
      "Epoch 130/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1708 - val_loss: 1.6253\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.70056\n",
      "Epoch 131/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1652 - val_loss: 1.6259\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.70056\n",
      "Epoch 132/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1711 - val_loss: 1.6304\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.70056\n",
      "Epoch 133/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1672 - val_loss: 1.6381\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.70056\n",
      "Epoch 134/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1668 - val_loss: 1.6382\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.70056\n",
      "Epoch 135/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1680 - val_loss: 1.6420\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.70056\n",
      "Epoch 136/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1653 - val_loss: 1.6293\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.70056\n",
      "Epoch 137/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1621 - val_loss: 1.6538\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.70056\n",
      "Epoch 138/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1627 - val_loss: 1.6534\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.70056\n",
      "Epoch 139/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1621 - val_loss: 1.6620\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.70056\n",
      "Epoch 140/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1612 - val_loss: 1.6627\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.70056\n",
      "Epoch 141/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1550 - val_loss: 1.6789\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.70056\n",
      "Epoch 142/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1583 - val_loss: 1.6697\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.70056\n",
      "Epoch 143/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1578 - val_loss: 1.6880\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.70056\n",
      "Epoch 144/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1500 - val_loss: 1.7119\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.70056\n",
      "Epoch 145/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1509 - val_loss: 1.7390\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.70056\n",
      "Epoch 146/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1539 - val_loss: 1.7170\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.70056\n",
      "Epoch 147/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1530 - val_loss: 1.7102\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.70056\n",
      "Epoch 148/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1499 - val_loss: 1.7316\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.70056\n",
      "Epoch 149/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1482 - val_loss: 1.7232\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.70056\n",
      "Epoch 150/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1451 - val_loss: 1.7431\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.70056\n",
      "Epoch 151/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1491 - val_loss: 1.7356\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.70056\n",
      "Epoch 152/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1452 - val_loss: 1.7682\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.70056\n",
      "Epoch 153/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1498 - val_loss: 1.7652\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.70056\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1431 - val_loss: 1.7478\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.70056\n",
      "Epoch 155/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1452 - val_loss: 1.7656\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.70056\n",
      "Epoch 156/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1417 - val_loss: 1.7671\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.70056\n",
      "Epoch 157/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1399 - val_loss: 1.7939\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.70056\n",
      "Epoch 158/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1424 - val_loss: 1.7722\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.70056\n",
      "Epoch 159/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1373 - val_loss: 1.7860\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.70056\n",
      "Epoch 160/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1337 - val_loss: 1.8073\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.70056\n",
      "Epoch 161/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1366 - val_loss: 1.7772\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.70056\n",
      "Epoch 162/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1344 - val_loss: 1.7944\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.70056\n",
      "Epoch 163/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1383 - val_loss: 1.8110\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.70056\n",
      "Epoch 164/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1370 - val_loss: 1.8063\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.70056\n",
      "Epoch 165/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1365 - val_loss: 1.8023\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.70056\n",
      "Epoch 166/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1290 - val_loss: 1.8314\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.70056\n",
      "Epoch 167/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1308 - val_loss: 1.8187\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.70056\n",
      "Epoch 168/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1315 - val_loss: 1.8194\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.70056\n",
      "Epoch 169/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1321 - val_loss: 1.8104\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.70056\n",
      "Epoch 170/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1281 - val_loss: 1.8356\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.70056\n",
      "Epoch 171/1000\n",
      "962/962 [==============================] - 20s 21ms/step - loss: 0.1306 - val_loss: 1.8321\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.70056\n",
      "Epoch 172/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1270 - val_loss: 1.8367\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.70056\n",
      "Epoch 173/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1234 - val_loss: 1.8580\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.70056\n",
      "Epoch 174/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1253 - val_loss: 1.8961\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.70056\n",
      "Epoch 175/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1237 - val_loss: 1.8692\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.70056\n",
      "Epoch 176/1000\n",
      "962/962 [==============================] - 21s 21ms/step - loss: 0.1231 - val_loss: 1.9157\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.70056\n",
      "Epoch 177/1000\n",
      "947/962 [============================>.] - ETA: 0s - loss: 0.1202"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-02e4e09f5372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearlyStop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodelCheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelCheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlyStop = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=30, verbose=1)\n",
    "modelCheck = keras.callbacks.ModelCheckpoint(\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\", save_best_only=True, verbose=1)\n",
    "history = model.fit_generator(gen, validation_data=(val_gen), epochs=1000, callbacks=[earlyStop, modelCheck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"Train\", \"Valid\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnly.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(encoded_test.reshape((-1,125*32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_pred.argmax(1), test_labels[0][:len(encoded_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49972976051856816"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_pred.argmax(1), test_labels[0][:len(encoded_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(encoded_train.reshape((-1, 125*32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.98      0.73    112517\n",
      "           1       0.86      0.15      0.25     91259\n",
      "\n",
      "    accuracy                           0.61    203776\n",
      "   macro avg       0.72      0.56      0.49    203776\n",
      "weighted avg       0.71      0.61      0.52    203776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_pred.argmax(1), train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5634694815859486"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_pred.argmax(1), train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other approaches for further feature analysis from Autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(encoded_train.reshape((-1, 125*32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aced33576273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastICA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/decomposition/fastica_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \"\"\"\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/decomposition/fastica_.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, compute_sources)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             compute_sources=compute_sources, return_n_iter=True)\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/decomposition/fastica_.py\u001b[0m in \u001b[0;36mfastica\u001b[0;34m(X, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, random_state, return_X_mean, compute_sources, return_n_iter)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'parallel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ica_par\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'deflation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ica_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/decomposition/fastica_.py\u001b[0m in \u001b[0;36m_ica_par\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mgwtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         W1 = _sym_decorrelation(np.dot(gwtx, X.T) / p_\n\u001b[0;32m--> 109\u001b[0;31m                                 - g_wtx[:, np.newaxis] * W)\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mgwtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# builtin max, abs are faster than numpy counter parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/decomposition/fastica_.py\u001b[0m in \u001b[0;36m_sym_decorrelation\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m^\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# u (resp. s) contains the eigenvectors (resp. square roots of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# the eigenvalues) of W * W.T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             w, v, info = evr(a1, uplo=uplo, jobz=_job, range=\"A\", il=1,\n\u001b[0;32m--> 432\u001b[0;31m                              iu=a1.shape[0], overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ica = FastICA()\n",
    "ica.fit(encoded_train.reshape((-1, 125*32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.013694\n",
       "1    0.009632\n",
       "2    0.009336\n",
       "3    0.009239\n",
       "4    0.008088\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pca.explained_variance_ratio_).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all elements of broadcast shape must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ba47ba2b8b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    180\u001b[0m            [1, 2, 3]])\n\u001b[1;32m    181\u001b[0m     \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot broadcast a non-scalar to a scalar array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         raise ValueError('all elements of broadcast shape must be non-'\n\u001b[0m\u001b[1;32m    123\u001b[0m                          'negative')\n\u001b[1;32m    124\u001b[0m     \u001b[0mneeds_writeable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreadonly\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all elements of broadcast shape must be non-negative"
     ]
    }
   ],
   "source": [
    "np.broadcast_to(keras.utils.to_categorical([0,1,0,0,1],2), (5,5,2)).transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try a sequence only network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seizure_time_labels = np.broadcast_to(keras.utils.to_categorical(train_labels[0], 2), (125, len(train_labels[0]), 2), ).transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203776, 125, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seizure_time_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seizure_time_labels = keras.utils.to_categorical(valid_labels[0][:len(encoded_valid)], 2)\n",
    "valid_seizure_time_labels = np.broadcast_to(valid_seizure_time_labels, (125, len(encoded_valid), 2)).transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7345d6e55303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseizure_time_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seizure_time_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, sample_weight, sampler, batch_size, keep_sparse, random_state)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mset_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_indices_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             raise ValueError(\"'sampler' needs to have an attribute \"\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "gen = imblearn.keras.BalancedBatchGenerator(encoded_train.reshape((-1,125*32)), seizure_time_labels, )\n",
    "val_gen = imblearn.keras.BalancedBatchGenerator(encoded_valid.reshape((-1,32*125)), valid_seizure_time_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ms994/miniconda3/envs/keras-redo/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ms994/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 125, 32)           8448      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 125, 32)           8448      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 125, 32)           1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 125, 32)           1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 125, 2)            66        \n",
      "=================================================================\n",
      "Total params: 19,714\n",
      "Trainable params: 19,394\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = keras.layers.Input((125*32,))\n",
    "y = keras.layers.Reshape((125,32))(x)\n",
    "for i in range(2):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.CuDNNLSTM(32, return_sequences=True)(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "for i in range(2):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.Dense(32)(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "y = keras.layers.BatchNormalization()(y)\n",
    "y = keras.layers.Dense(2, activation=\"softmax\")(y)\n",
    "model = keras.models.Model(inputs=x, outputs=y)\n",
    "model.compile(keras.optimizers.Adam(lr=0.00005), loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203776, 4000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train.reshape((-1,125*32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ms994/miniconda3/envs/keras-redo/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ms994/miniconda3/envs/keras-redo/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 203776 samples, validate on 81152 samples\n",
      "Epoch 1/1000\n",
      "203776/203776 [==============================] - 133s 655us/step - loss: 0.4285 - val_loss: 0.2251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22514, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 2/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2760 - val_loss: 0.2232\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22514 to 0.22325, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 3/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2730 - val_loss: 0.2230\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22325 to 0.22300, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 4/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2720 - val_loss: 0.2225\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22300 to 0.22254, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 5/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2714 - val_loss: 0.2228\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22254\n",
      "Epoch 6/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2710 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22254 to 0.22226, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 7/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2707 - val_loss: 0.2227\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22226\n",
      "Epoch 8/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2705 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22226\n",
      "Epoch 9/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2703 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22226 to 0.22171, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 10/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2702 - val_loss: 0.2216\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22171 to 0.22157, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 11/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2702 - val_loss: 0.2221\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22157\n",
      "Epoch 12/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2700 - val_loss: 0.2219\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22157\n",
      "Epoch 13/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2698 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22157\n",
      "Epoch 14/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2697 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22157\n",
      "Epoch 15/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2696 - val_loss: 0.2215\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.22157 to 0.22152, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 16/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2695 - val_loss: 0.2215\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22152\n",
      "Epoch 17/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2694 - val_loss: 0.2214\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22152 to 0.22140, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 18/1000\n",
      "203776/203776 [==============================] - 127s 623us/step - loss: 0.2693 - val_loss: 0.2214\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22140\n",
      "Epoch 19/1000\n",
      "203776/203776 [==============================] - 127s 623us/step - loss: 0.2692 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22140\n",
      "Epoch 20/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2691 - val_loss: 0.2219\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22140\n",
      "Epoch 21/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2689 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22140 to 0.22107, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 22/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2687 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22107\n",
      "Epoch 23/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2685 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.22107\n",
      "Epoch 24/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2683 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22107\n",
      "Epoch 25/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2681 - val_loss: 0.2207\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.22107 to 0.22072, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 26/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2680 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.22072\n",
      "Epoch 27/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2679 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.22072\n",
      "Epoch 28/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2678 - val_loss: 0.2208\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.22072\n",
      "Epoch 29/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2678 - val_loss: 0.2204\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.22072 to 0.22040, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 30/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2678 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.22040\n",
      "Epoch 31/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2677 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.22040\n",
      "Epoch 32/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2677 - val_loss: 0.2201\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.22040 to 0.22014, saving model to /n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\n",
      "Epoch 33/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2677 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.22014\n",
      "Epoch 34/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2677 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.22014\n",
      "Epoch 35/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2677 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.22014\n",
      "Epoch 36/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2677 - val_loss: 0.2212\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.22014\n",
      "Epoch 37/1000\n",
      "203776/203776 [==============================] - 125s 613us/step - loss: 0.2677 - val_loss: 0.2215\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.22014\n",
      "Epoch 38/1000\n",
      "203776/203776 [==============================] - 126s 617us/step - loss: 0.2677 - val_loss: 0.2210\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.22014\n",
      "Epoch 39/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2677 - val_loss: 0.2210\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.22014\n",
      "Epoch 40/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2677 - val_loss: 0.2212\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22014\n",
      "Epoch 41/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2676 - val_loss: 0.2216\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22014\n",
      "Epoch 42/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2676 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22014\n",
      "Epoch 43/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2677 - val_loss: 0.2216\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22014\n",
      "Epoch 44/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2676 - val_loss: 0.2206\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22014\n",
      "Epoch 45/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2676 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22014\n",
      "Epoch 46/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2676 - val_loss: 0.2216\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22014\n",
      "Epoch 47/1000\n",
      "203776/203776 [==============================] - 127s 621us/step - loss: 0.2676 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.22014\n",
      "Epoch 48/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2676 - val_loss: 0.2215\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22014\n",
      "Epoch 49/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2676 - val_loss: 0.2214\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22014\n",
      "Epoch 50/1000\n",
      "203776/203776 [==============================] - 126s 621us/step - loss: 0.2675 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22014\n",
      "Epoch 51/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2675 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22014\n",
      "Epoch 52/1000\n",
      "203776/203776 [==============================] - 127s 622us/step - loss: 0.2675 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.22014\n",
      "Epoch 53/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2674 - val_loss: 0.2222\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22014\n",
      "Epoch 54/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2673 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.22014\n",
      "Epoch 55/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2673 - val_loss: 0.2220\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.22014\n",
      "Epoch 56/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2672 - val_loss: 0.2215\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.22014\n",
      "Epoch 57/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2671 - val_loss: 0.2224\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.22014\n",
      "Epoch 58/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2671 - val_loss: 0.2224\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.22014\n",
      "Epoch 59/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2670 - val_loss: 0.2224\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.22014\n",
      "Epoch 60/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2670 - val_loss: 0.2226\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.22014\n",
      "Epoch 61/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2668 - val_loss: 0.2225\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.22014\n",
      "Epoch 62/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2667 - val_loss: 0.2220\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.22014\n",
      "Epoch 63/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2667 - val_loss: 0.2233\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.22014\n",
      "Epoch 64/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2666 - val_loss: 0.2244\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.22014\n",
      "Epoch 65/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2665 - val_loss: 0.2237\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.22014\n",
      "Epoch 66/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2664 - val_loss: 0.2227\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.22014\n",
      "Epoch 67/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2663 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.22014\n",
      "Epoch 68/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2663 - val_loss: 0.2232\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.22014\n",
      "Epoch 69/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2662 - val_loss: 0.2243\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.22014\n",
      "Epoch 70/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2661 - val_loss: 0.2235\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.22014\n",
      "Epoch 71/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2660 - val_loss: 0.2238\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.22014\n",
      "Epoch 72/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2659 - val_loss: 0.2237\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.22014\n",
      "Epoch 73/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2658 - val_loss: 0.2238\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.22014\n",
      "Epoch 74/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2658 - val_loss: 0.2233\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.22014\n",
      "Epoch 75/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2657 - val_loss: 0.2223\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.22014\n",
      "Epoch 76/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2655 - val_loss: 0.2237\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.22014\n",
      "Epoch 77/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2655 - val_loss: 0.2231\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.22014\n",
      "Epoch 78/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2653 - val_loss: 0.2251\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.22014\n",
      "Epoch 79/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2653 - val_loss: 0.2255\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.22014\n",
      "Epoch 80/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2651 - val_loss: 0.2248\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.22014\n",
      "Epoch 81/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2650 - val_loss: 0.2251\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.22014\n",
      "Epoch 82/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2650 - val_loss: 0.2250\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.22014\n",
      "Epoch 83/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2649 - val_loss: 0.2246\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.22014\n",
      "Epoch 84/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2647 - val_loss: 0.2255\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.22014\n",
      "Epoch 85/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2647 - val_loss: 0.2253\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.22014\n",
      "Epoch 86/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2644 - val_loss: 0.2245\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.22014\n",
      "Epoch 87/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2646 - val_loss: 0.2257\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.22014\n",
      "Epoch 88/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2644 - val_loss: 0.2274\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.22014\n",
      "Epoch 89/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2644 - val_loss: 0.2257\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.22014\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2643 - val_loss: 0.2250\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.22014\n",
      "Epoch 91/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2642 - val_loss: 0.2249\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.22014\n",
      "Epoch 92/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2640 - val_loss: 0.2284\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.22014\n",
      "Epoch 93/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2639 - val_loss: 0.2291\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.22014\n",
      "Epoch 94/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2637 - val_loss: 0.2281\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.22014\n",
      "Epoch 95/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2637 - val_loss: 0.2266\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.22014\n",
      "Epoch 96/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2637 - val_loss: 0.2265\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.22014\n",
      "Epoch 97/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2637 - val_loss: 0.2264\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.22014\n",
      "Epoch 98/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2636 - val_loss: 0.2272\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.22014\n",
      "Epoch 99/1000\n",
      "203776/203776 [==============================] - 126s 620us/step - loss: 0.2634 - val_loss: 0.2268\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.22014\n",
      "Epoch 100/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2634 - val_loss: 0.2275\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.22014\n",
      "Epoch 101/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2633 - val_loss: 0.2273\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.22014\n",
      "Epoch 102/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2632 - val_loss: 0.2269\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.22014\n",
      "Epoch 103/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2630 - val_loss: 0.2276\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.22014\n",
      "Epoch 104/1000\n",
      "203776/203776 [==============================] - 126s 619us/step - loss: 0.2631 - val_loss: 0.2283\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.22014\n",
      "Epoch 105/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2631 - val_loss: 0.2293\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.22014\n",
      "Epoch 106/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2630 - val_loss: 0.2270\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.22014\n",
      "Epoch 107/1000\n",
      "203776/203776 [==============================] - 126s 618us/step - loss: 0.2627 - val_loss: 0.2271\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.22014\n",
      "Epoch 108/1000\n",
      "203776/203776 [==============================] - 125s 613us/step - loss: 0.2628 - val_loss: 0.2302\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.22014\n",
      "Epoch 109/1000\n",
      "203776/203776 [==============================] - 123s 606us/step - loss: 0.2628 - val_loss: 0.2286\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.22014\n",
      "Epoch 110/1000\n",
      "203776/203776 [==============================] - 124s 608us/step - loss: 0.2625 - val_loss: 0.2294\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.22014\n",
      "Epoch 111/1000\n",
      "203776/203776 [==============================] - 123s 606us/step - loss: 0.2625 - val_loss: 0.2288\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.22014\n",
      "Epoch 112/1000\n",
      "203776/203776 [==============================] - 123s 605us/step - loss: 0.2624 - val_loss: 0.2279\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.22014\n",
      "Epoch 113/1000\n",
      "203776/203776 [==============================] - 123s 606us/step - loss: 0.2623 - val_loss: 0.2297\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.22014\n",
      "Epoch 114/1000\n",
      "203776/203776 [==============================] - 124s 607us/step - loss: 0.2622 - val_loss: 0.2275\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.22014\n",
      "Epoch 115/1000\n",
      "203776/203776 [==============================] - 124s 607us/step - loss: 0.2621 - val_loss: 0.2281\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.22014\n",
      "Epoch 116/1000\n",
      "203776/203776 [==============================] - 124s 606us/step - loss: 0.2620 - val_loss: 0.2284\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.22014\n",
      "Epoch 117/1000\n",
      "203776/203776 [==============================] - 124s 607us/step - loss: 0.2621 - val_loss: 0.2286\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.22014\n",
      "Epoch 118/1000\n",
      "203776/203776 [==============================] - 124s 608us/step - loss: 0.2619 - val_loss: 0.2280\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.22014\n",
      "Epoch 119/1000\n",
      " 75520/203776 [==========>...................] - ETA: 1:09 - loss: 0.2616"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d48a3c0991a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearlyStop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodelCheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseizure_time_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seizure_time_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelCheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlyStop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, verbose=1)\n",
    "modelCheck = keras.callbacks.ModelCheckpoint(\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\", save_best_only=True, verbose=1)\n",
    "history = model.fit(encoded_train.reshape((-1,125*32)), seizure_time_labels, batch_size=32, validation_data=(encoded_valid.reshape((-1,125*32)), valid_seizure_time_labels), epochs=1000, callbacks=[earlyStop, modelCheck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"/n/scratch2/ms994/autoEncoderOutputCNNLSTMSeizureOnlyOverTime.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(encoded_test.reshape((-1,125*32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81152, 125, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f05b03177034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \"\"\"\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-redo/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and multiclass-multioutput targets"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels[0][:len(encoded_test)], pred.argmax(1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
